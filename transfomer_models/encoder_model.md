# エンコーダーモデル

エンコーダーモデルは、トランスフォーマーモデルのエンコーダーのみを使用します。各段階で、注意層は初期の文のすべての単語にアクセスできます。これらのモデルは、双方向の注意を持つことで特徴付けられ、しばしば自己符号化モデルと呼ばれます。これらのモデルの目的は、通常、与えられた文をいかにして何らかの形で破損させるか（例えば、その中のランダムな単語をマスクするなど）であり、その後モデルに元の文を見つけたり再構築したりするように課題を与えることです。エンコーダーモデルは、文全体の理解が必要なタスクに最適であり、文の分類、固有表現認識、より一般的には単語の分類や抽出型質問応答などのタスクに適しています。このモデルファミリーの代表には、ALBERT、BERT、DistillBERT、ELECTRA、RoBERTaが含まれます。